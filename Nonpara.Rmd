---
title: "Nonparametric Statistics"
author: "Danyu Zhang & Daniel Alonso"
date: "2021/3/27"
output: pdf_document
---


### A.6. Exercise 5.11. 

```{r}
challenger <- read.table("Challenger.txt", header = TRUE)
```

The challenger.txt dataset contains information regarding the state of the solid rocket boosters after launch for 23 shuttle flights prior the Challenger launch. Each row has, among others, the variables fail.field (indicator of whether there was an incident with the O-rings), nfail.field (number of incidents with the O-rings), and temp (temperature in the day of launch, measured in Celsius degrees). 

a). Fit a local logistic regression (first degree) for fails.field ~ temp, for three choices of bandwidths: one that oversmooths, another that is somehow adequate, and another that undersmooths. Do the effects of temp on fails.field seem to be significant?

We can observe roughly that the lower the temperature, the higher the probability of having a incident with the O-rings as the most of the observations that have temperatures below 20 have had an incident; while the majority of the observations that have temperatures higher than 20 have not had any incidents. 

```{r}
X = challenger$temp
Y = challenger$fail.field

plot(X, Y)
```

First of all, we will define a logistic function which transforms $$\beta_0$$ to probability:
```{r}
logistic <- function(x) 1 / (1 + exp(-x))
```

- **Bandwidth that undersmooths**: h=0.2

When the bandwidths equal to 0.2, we can notice that the local logistic regression is roughly interpolating the data points that we have as samples which means that, the estimator is overfitting the samples. 
```{r}
h <- 0.2
x <- seq(10, 28, l = 5000)

suppressWarnings(
  fit_glm <- sapply(x, function(x) {
    K <- dnorm(x = x, mean = X, sd = h)
    glm.fit(x = cbind(1, X - x), y = Y, weights = K,
            family = binomial())$coefficients[1]
  })
)

plot(x, logistic(fit_glm))
```
- **Bandwidth that oversmooths**: h=10

If we set the bandwidth to 10, it is somehow very clear to see that the local likelihood estimator oversmooths the samples, which makes that the samples lose their characteristics.  

```{r}
h <- 10
x <- seq(10, 28, l = 5000)

suppressWarnings(
  fit_glm <- sapply(x, function(x) {
    K <- dnorm(x = x, mean = X, sd = h)
    glm.fit(x = cbind(1, X - x), y = Y, weights = K,
            family = binomial())$coefficients[1]
  })
)

plot(x, logistic(fit_glm))
```
- **Adequate Bandwidth**: h=2

By setting the bandwidth to 2, we can observe that the regression is more or less smooth, and it fits the data set that we have without interpolating it or oversmooth is. 

```{r}
h <- 2
x <- seq(10, 28, l = 5000)

suppressWarnings(
  fit_glm <- sapply(x, function(x) {
    K <- dnorm(x = x, mean = X, sd = h)
    glm.fit(x = cbind(1, X - x), y = Y, weights = K,
            family = binomial())$coefficients[1]
  })
)

plot(x, logistic(fit_glm))
```


b). Obtain hˆLCV and plot the LCV function with a reasonable accuracy. 

In this part, the aim is to obtain the best bandwidth by maximazing the likelihood function. As seen before, we know that the optimal bandwidth value will be approximately around 2, so it is reasonable to try the different values around it. 

By plotting the likelihood in function of the possible values of bandwidths, we can see that, 2 is the optimal bandwidth value for the data set. 

```{r}
n <- length(Y)
h <- seq(1.5, 2.3, by=0.1)

suppressWarnings(
  LCV <- sapply(h, function(h) {
  sum(sapply(1:n, function(i) {
    K <- dnorm(x = X[i], mean = X[-i], sd = h)
    nlm(f = function(beta) {
      -sum(K * (Y[-i] * (beta[1] + beta[2] * (X[-i] - X[i])) -
                  log(1 + exp(beta[1] + beta[2] * (X[-i] - X[i])))))
      }, p = c(0, 0))$minimum
    }))
  })
)
plot(h, LCV, type = "o")
abline(v = h[which.max(LCV)], col = 2)
```

c). Using hˆLCV, predict the probability of an incident at temperatures −0.6 (launch temperature of the Challenger) and 11.67 (specific recommendation by the vice president of engineers).

```{r}
h <- 2
x <- seq(-0.6, 11.67, l = 500)

suppressWarnings(
  fit_glm <- sapply(x, function(x) {
    K <- dnorm(x = x, mean = X, sd = h)
    glm.fit(x = cbind(1, X - x), y = Y, weights = K,
            family = binomial())$coefficients[1]
  })
)

plot(x, logistic(fit_glm))

# Find probability at x=-0.6 and x=11.67
pred0.6 = logistic(fit_glm)[1]
pred11.67 = logistic(fit_glm)[length(fit_glm)]

pred0.6
pred11.67
```

d). What are the local odds at −0.6 and 11.67? Show the local logistic models about these points, in spirit of Figure 5.1, and interpret the results. 

```{r}
h <- 2
x <- -0.6

suppressWarnings(
  fit_glm <- sapply(x, function(x) {
    K <- dnorm(x = x, mean = X, sd = h)
    glm.fit(x = cbind(1, X - x), y = Y, weights = K,
            family = binomial())$coefficients[1]
  })
)

fit_glm
```


### C.4. Exercise 3.30. Load the ovals.RData file.

a). Split the dataset into the training sample, comprised of the first 2000 observations, and the test sample (rest of the sample). Plot the dataset with colors for its classes. What can you say about the classification problem?

We can see roughly that the distribution of the observations of the third class and the first/second class is very different, which means that the classification problem would be relatively easy between them (splitting the third class data from the first or the second data). But the distribution of the first class and the second class are very similar observing the plot, only that the distribution of the first class observations are more dispersed, the shape is very likely to be the same, so the classification problem between those two classes will probably be harder. 

```{r}
ovals$labels = as.factor(ovals$labels)
train <- ovals[1:2000,]
test <- ovals[2001:3000,]

library(ggplot2)
ggplot(data = train, aes(x.1, x.2, color = labels)) +
  geom_point() +   
  scale_color_manual(values = c("1" = "red", "2" = "blue", "3" = "yellow"))

```

b). Using the training sample, compute the plug-in bandwidth matrices for all the classes.

```{r}
library(ks)
library(dplyr)
x1 <- ovals %>% filter(labels=="1") %>%
  select(x.1,x.2)
x2 <- ovals %>% filter(labels=="2") %>%
  select(x.1,x.2)
x3 <- ovals %>% filter(labels=="3") %>%
  select(x.1,x.2)

h1 <- ks::Hpi(x1)
h2 <- ks::Hpi(x2)
h3 <- ks::Hpi(x3)
```

As observed before, we can observe that the border of the first class if more blurry than the other 2 classes due to the fact that the observations are more dispersed; also, the shape of the estimated distributions of group 1 and group 2 are very similar while the estimated distribution of the third group is very different. 

```{r}
cont <- seq(0, 0.05, l = 20)
col <- viridis::viridis

par(mfrow = c(1, 3))

plot(ks::kde(x = x1, H = h1), display = "filled.contour2",
     abs.cont = cont, col.fun = col, main = "Plug in bandwidth for group1")
plot(ks::kde(x = x2, H = h2), display = "filled.contour2",
     abs.cont = cont, col.fun = col, main = "Plug in bandwidth for group2")
plot(ks::kde(x = x3, H = h3), display = "filled.contour2",
     abs.cont = cont, col.fun = col, main = "Plug in bandwidth for group3")
```

We can observe that the estimation of the distribution with diagonal bandwidths seem to be very likely to be the same as non-diagonal bandwidths. 
```{r}
h1_diag <- ks::Hpi.diag(x1)
h2_diag <- ks::Hpi.diag(x2)
h3_diag <- ks::Hpi.diag(x3)

par(mfrow = c(1, 3))

plot(ks::kde(x = x1, H = h1_diag), display = "filled.contour2",
     abs.cont = cont, col.fun = col, main = "Diagonal Plug in bandwidth for group1")
plot(ks::kde(x = x2, H = h2_diag), display = "filled.contour2",
     abs.cont = cont, col.fun = col, main = "Diagonal Plug in bandwidth for group2")
plot(ks::kde(x = x3, H = h3_diag), display = "filled.contour2",
     abs.cont = cont, col.fun = col, main = "Diagonal Plug in bandwidth for group3")
```

c).Use these plug-in bandwidths to perform kernel discriminant analysis.

First we split the predictors from the response in order to fit the model. 
```{r}
x <- train[,1:2]
groups<-train$labels
```

Then we use *kda* function from package *ks* which computes automatically the model selecting the plug in bandwidths. 
```{r}
#Hpi bandwidths are computed
kda <- ks::kda(x=x, x.group = groups) 
```

d). Plot the contours of the kernel density estimator of each class and the classes partitions. Use coherent colors between contours and points.

We can see that the areas classified as red/green coincide, as the highest density red/green areas correspond to the same groups of points, however, most of these were classified in the green group, as the density is significantly more concentrated in the green area. 

The remaining points around the green/blue areas and the middle part are classified inside the red group.

```{r}
plot(kda, col = rainbow(3), lwd = 2, col.pt = 1, cont = seq(5, 85, by = 20), col.part = rainbow(3, alpha = 0.25), drawpoints = TRUE)
```

e). Predict the class for the test sample and compare with the true classes. Then report the successful classification rate.

From the report generated by *compare* from package *ks*, we can see that the kda classifier has an accuracy of **0.717%**, it classifies specially well the first group (accuracy of 96/97=**0.98%**), while it confuses the most the second class with the first one. 
```{r}
new_data=test[1:2]
```

```{r}
pred_kda <- predict(kda, x = new_data)
ks::compare(x.group = test$labels, est.group = pred_kda)
1-0.283
```

f). Compare the successful classification rate with the one given by LDA. Is it better than kernel discriminant analysis? 

The kernel discriminant analysis is better. 
```{r}
lda_model = MASS::lda(x, grouping = groups, data=train)
pred_lda = predict(lda_model, newdata = new_data)$class
ks::compare(test$labels, pred_lda)
```

g). Repeat f with QDA.

The kernel discriminant analysis is better.
```{r}
qda_model = MASS::qda(x, grouping = groups, data=train)
pred_qda = predict(qda_model, newdata =new_data)$class
ks::compare(test$labels, pred_qda)
```

